---
title: logstash docs for outputs/google_bigquery
layout: content_right
---
<h2>google_bigquery</h2>
<h3>Milestone: <a href="../plugin-milestones">1</a></h3>

<p>Author: Rodrigo De Castro <a href="&#109;&#097;&#105;&#108;&#116;&#111;:&#114;&#100;&#099;&#064;&#103;&#111;&#111;&#103;&#108;&#101;&#046;&#099;&#111;&#109;">&#114;&#100;&#099;&#064;&#103;&#111;&#111;&#103;&#108;&#101;&#046;&#099;&#111;&#109;</a>
Date: 2013-09-20</p>

<p>Copyright 2013 Google Inc.</p>

<p>Licensed under the Apache License, Version 2.0 (the &#8220;License&#8221;);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at</p>

<pre><code> http://www.apache.org/licenses/LICENSE-2.0
</code></pre>

<p>Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an &#8220;AS IS&#8221; BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
Summary: plugin to upload log events to Google BigQuery (BQ), rolling
files based on the date pattern provided as a configuration setting. Events
are written to files locally and, once file is closed, this plugin uploads
it to the configured BigQuery dataset.</p>

<p>VERY IMPORTANT:
1 - To make good use of BigQuery, your log events should be parsed and
structured. Consider using grok to parse your events into fields that can
be uploaded to BQ.
2 - You must configure your plugin so it gets events with the same structure,
so the BigQuery schema suits them. In case you want to upload log events
with different structures, you can utilize multiple configuration blocks,
separating different log events with Logstash conditionals. More details on
Logstash conditionals can be found here:
http://logstash.net/docs/1.2.1/configuration#conditionals</p>

<p>For more info on Google BigQuery, please go to:
https://developers.google.com/bigquery/</p>

<p>In order to use this plugin, a Google service account must be used. For
more information, please refer to:
https://developers.google.com/storage/docs/authentication#service_accounts</p>

<p>Recommendations:
a - Experiment with the settings depending on how much log data you generate,
your needs to see &#8220;fresh&#8221; data, and how much data you could lose in the event
of crash. For instance, if you want to see recent data in BQ quickly, you
could configure the plugin to upload data every minute or so (provided you
have enough log events to justify that). Note also, that if uploads are too
frequent, there is no guarantee that they will be imported in the same order,
so later data may be available before earlier data.
b - BigQuery charges for storage and for queries, depending on how much data
it reads to perform a query. These are other aspects to consider when
considering the date pattern which will be used to create new tables and also
how to compose the queries when using BQ. For more info on BigQuery Pricing,
please access:
https://developers.google.com/bigquery/pricing</p>

<p>USAGE:
This is an example of logstash config:</p>

<p>output {
   google_bigquery {
     project_id =&gt; &#8220;folkloric-guru-278&#8221;                        (required)
     dataset =&gt; &#8220;logs&#8221;                                         (required)
     csv_schema =&gt; &#8220;path:STRING,status:INTEGER,score:FLOAT&#8221;    (required)
     key_path =&gt; &#8220;/path/to/privatekey.p12&#8221;                     (required)
     key_password =&gt; &#8220;notasecret&#8221;                              (optional)
     service_account =&gt; &#8220;1234@developer.gserviceaccount.com&#8221;   (required)
     temp_directory =&gt; &#8220;/tmp/logstash-bq&#8221;                      (optional)
     temp_file_prefix =&gt; &#8220;logstash_bq&#8221;                         (optional)
     date_pattern =&gt; &#8220;%Y-%m-%dT%H:00&#8221;                          (optional)
     flush_interval_secs =&gt; 2                                  (optional)
     uploader_interval_secs =&gt; 60                              (optional)
     deleter_interval_secs =&gt; 60                               (optional)
   }
}</p>

<p>Improvements TODO list:
- Refactor common code between Google BQ and GCS plugins.
- Turn Google API code into a Plugin Mixin (like AwsConfig).
- There&#8217;s no recover method, so if logstash/plugin crashes, files may not
be uploaded to BQ.</p>


<h3> Synopsis </h3>

This is what it might look like in your config file:

<pre><code>output {
  google_bigquery {
    <a href="#codec">codec</a> => ... # codec (optional), default: "plain"
    <a href="#csv_schema">csv_schema</a> => ... # string (required)
    <a href="#dataset">dataset</a> => ... # string (required)
    <a href="#date_pattern">date_pattern</a> => ... # string (optional), default: "%Y-%m-%dT%H:00"
    <a href="#deleter_interval_secs">deleter_interval_secs</a> => ... # number (optional), default: 60
    <a href="#flush_interval_secs">flush_interval_secs</a> => ... # number (optional), default: 2
    <a href="#key_password">key_password</a> => ... # string (optional), default: "notasecret"
    <a href="#key_path">key_path</a> => ... # string (required)
    <a href="#project_id">project_id</a> => ... # string (required)
    <a href="#service_account">service_account</a> => ... # string (required)
    <a href="#table_prefix">table_prefix</a> => ... # string (optional), default: "logstash"
    <a href="#temp_directory">temp_directory</a> => ... # string (optional), default: ""
    <a href="#temp_file_prefix">temp_file_prefix</a> => ... # string (optional), default: "logstash_bq"
    <a href="#uploader_interval_secs">uploader_interval_secs</a> => ... # number (optional), default: 60
    <a href="#workers">workers</a> => ... # number (optional), default: 1
  }
}
</code></pre>

<h3> Details </h3>

<h4> 
  <a name="codec">
    codec
    
</a>
</h4>

<ul>
  <li> Value type is <a href="../configuration#codec">codec</a> </li>
  <li> Default value is "plain" </li>
</ul>

<p>The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.</p>


<h4> 
  <a name="csv_schema">
    csv_schema (required setting)
    
</a>
</h4>

<ul>
  <li> Value type is <a href="../configuration#string">string</a> </li>
  <li> There is no default value for this setting. </li>
</ul>

<p>Schema for log data. It must follow this format:</p>
<field1-name>:<field1-type>,<field2-name>:<field2-type>,...
Example: path:STRING,status:INTEGER,score:FLOAT
</field2-type></field2-name></field1-type></field1-name>


<h4> 
  <a name="dataset">
    dataset (required setting)
    
</a>
</h4>

<ul>
  <li> Value type is <a href="../configuration#string">string</a> </li>
  <li> There is no default value for this setting. </li>
</ul>

<p>BigQuery dataset to which these events will be added to.</p>


<h4> 
  <a name="date_pattern">
    date_pattern
    
</a>
</h4>

<ul>
  <li> Value type is <a href="../configuration#string">string</a> </li>
  <li> Default value is "%Y-%m-%dT%H:00" </li>
</ul>

<p>Time pattern for BigQuery table, defaults to hourly tables.
Must Time.strftime patterns: www.ruby-doc.org/core-2.0/Time.html#method-i-strftime</p>


<h4> 
  <a name="deleter_interval_secs">
    deleter_interval_secs
    
</a>
</h4>

<ul>
  <li> Value type is <a href="../configuration#number">number</a> </li>
  <li> Default value is 60 </li>
</ul>

<p>Deleter interval when checking if upload jobs are done for file deletion.
This only affects how long files are on the hard disk after the job is done.</p>


<h4> 
  <a name="exclude_tags">
    exclude_tags
     <strong>DEPRECATED</strong>
</a>
</h4>

<ul>
  <li> DEPRECATED WARNING: This config item is deprecated. It may be removed in a further version. </li>
  <li> Value type is <a href="../configuration#array">array</a> </li>
  <li> Default value is [] </li>
</ul>

<p>Only handle events without any of these tags. Note this check is additional to type and tags.</p>


<h4> 
  <a name="flush_interval_secs">
    flush_interval_secs
    
</a>
</h4>

<ul>
  <li> Value type is <a href="../configuration#number">number</a> </li>
  <li> Default value is 2 </li>
</ul>

<p>Flush interval in seconds for flushing writes to log files. 0 will flush
on every message.</p>


<h4> 
  <a name="key_password">
    key_password
    
</a>
</h4>

<ul>
  <li> Value type is <a href="../configuration#string">string</a> </li>
  <li> Default value is "notasecret" </li>
</ul>

<p>Private key password for service account private key.</p>


<h4> 
  <a name="key_path">
    key_path (required setting)
    
</a>
</h4>

<ul>
  <li> Value type is <a href="../configuration#string">string</a> </li>
  <li> There is no default value for this setting. </li>
</ul>

<p>Path to private key file for Google Service Account.</p>


<h4> 
  <a name="project_id">
    project_id (required setting)
    
</a>
</h4>

<ul>
  <li> Value type is <a href="../configuration#string">string</a> </li>
  <li> There is no default value for this setting. </li>
</ul>

<p>Google Cloud Project ID (number, not Project Name!).</p>


<h4> 
  <a name="service_account">
    service_account (required setting)
    
</a>
</h4>

<ul>
  <li> Value type is <a href="../configuration#string">string</a> </li>
  <li> There is no default value for this setting. </li>
</ul>

<p>Service account to access Google APIs.</p>


<h4> 
  <a name="table_prefix">
    table_prefix
    
</a>
</h4>

<ul>
  <li> Value type is <a href="../configuration#string">string</a> </li>
  <li> Default value is "logstash" </li>
</ul>

<p>BigQuery table ID prefix to be used when creating new tables for log data.
Table name will be <table_prefix>_<date></date></table_prefix></p>


<h4> 
  <a name="tags">
    tags
     <strong>DEPRECATED</strong>
</a>
</h4>

<ul>
  <li> DEPRECATED WARNING: This config item is deprecated. It may be removed in a further version. </li>
  <li> Value type is <a href="../configuration#array">array</a> </li>
  <li> Default value is [] </li>
</ul>

<p>Only handle events with all of these tags.  Note that if you specify
a type, the event must also match that type.
Optional.</p>


<h4> 
  <a name="temp_directory">
    temp_directory
    
</a>
</h4>

<ul>
  <li> Value type is <a href="../configuration#string">string</a> </li>
  <li> Default value is "" </li>
</ul>

<p>Directory where temporary files are stored.
Defaults to /tmp/logstash-bq-<random-suffix></random-suffix></p>


<h4> 
  <a name="temp_file_prefix">
    temp_file_prefix
    
</a>
</h4>

<ul>
  <li> Value type is <a href="../configuration#string">string</a> </li>
  <li> Default value is "logstash_bq" </li>
</ul>

<p>Temporary local file prefix. Log file will follow the format:</p>
<prefix>_hostname_date.part?.log
</prefix>


<h4> 
  <a name="type">
    type
     <strong>DEPRECATED</strong>
</a>
</h4>

<ul>
  <li> DEPRECATED WARNING: This config item is deprecated. It may be removed in a further version. </li>
  <li> Value type is <a href="../configuration#string">string</a> </li>
  <li> Default value is "" </li>
</ul>

<p>The type to act on. If a type is given, then this output will only
act on messages with the same type. See any input plugin&#8217;s &#8220;type&#8221;
attribute for more.
Optional.</p>


<h4> 
  <a name="uploader_interval_secs">
    uploader_interval_secs
    
</a>
</h4>

<ul>
  <li> Value type is <a href="../configuration#number">number</a> </li>
  <li> Default value is 60 </li>
</ul>

<p>Uploader interval when uploading new files to BigQuery. Adjust time based
on your time pattern (for example, for hourly files, this interval can be
around one hour).</p>


<h4> 
  <a name="workers">
    workers
    
</a>
</h4>

<ul>
  <li> Value type is <a href="../configuration#number">number</a> </li>
  <li> Default value is 1 </li>
</ul>

<p>The number of workers to use for this output.
Note that this setting may not be useful for all outputs.</p>



<hr>

This is documentation from <a href="https://github.com/logstash/logstash/blob/v1.4.5/lib/logstash/outputs/google_bigquery.rb">lib/logstash/outputs/google_bigquery.rb</a>
